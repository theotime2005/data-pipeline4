# ğŸ“ SYNTHÃˆSE DE CONSTRUCTION - Data Pipeline Iris

## âœ… Statut : **COMPLET - PRÃŠT POUR LA SOUTENANCE**

---

## ğŸ“¦ FICHIERS CRÃ‰Ã‰S / CONFIGURÃ‰S

### 1. **Configuration & Environnement**
- `.env` - Variables d'environnement centralisÃ©es (DB, MLflow)
- `docker-compose.yml` - Orchestration 5 services (validÃ© âœ“)
- `launch.sh` - Script bash pour dÃ©marrage rapide

### 2. **Database PostgreSQL**
- `db/init.sql` - SchÃ©ma : table `iris_clean` (5 colonnes)

### 3. **Service Preprocess**
- `services/preprocess/Dockerfile` - Python 3.11-slim + pip install
- `services/preprocess/preprocess.py` - ETL logic (csv â†’ clean â†’ insert)
- `services/preprocess/requirements.txt` - pandas, SQLAlchemy, scikit-learn

### 4. **Service Training**
- `services/train/Dockerfile` - Python 3.11-slim
- `services/train/train.py` - RandomForestRegressor + MLflow logging
- `services/train/requirements.txt` - scikit-learn, MLflow

### 5. **Service API FastAPI**
- `services/api/Dockerfile` - FastAPI + Uvicorn
- `services/api/main.py` - Endpoints: /health, /predict
- `services/api/requirements.txt` - FastAPI, Pydantic, MLflow client

### 6. **Dataset**
- `iris.csv` - 150 observations (50 setosa, 50 versicolor, 50 virginica)

### 7. **Documentation**
- `README.md` - Guide complet (architecture, dÃ©marrage, soutenance)
- `SYNTHÃˆSE.md` - Ce fichier

---

## ğŸ¯ VALIDATION

| Composant | Status | VÃ©rification |
|-----------|--------|------------|
| docker-compose.yml | âœ… | `docker compose config` PASS |
| preprocess.py | âœ… | `python3 -m py_compile` PASS |
| train.py | âœ… | `python3 -m py_compile` PASS |
| main.py (API) | âœ… | `python3 -m py_compile` PASS |
| iris.csv | âœ… | 151 lignes (150 data + 1 header) |

---

## ğŸš€ DÃ‰MARRAGE

### DÃ©marrage complet
```bash
docker compose up --build
```

### Ou avec le script
```bash
bash launch.sh
```

### Temps d'attente estimÃ©
- DB ready: ~5-10s
- MLflow server up: ~20-30s
- Preprocess done: ~30-40s
- Training done: ~1-2 min
- API ready: ~5s aprÃ¨s training

**Total estimÃ©: 2-3 minutes**

---

## ğŸ“ ACCÃˆS APRÃˆS DÃ‰MARRAGE

| Interface | URL | Fonction |
|-----------|-----|----------|
| MLflow | `http://localhost:5000` | Voir expÃ©riences + mÃ©triques |
| API Swagger | `http://localhost:8000/docs` | Tester les endpoints |
| API Health | `http://localhost:8000/health` | VÃ©rifier le statut |

---

## ğŸ§ª TEST RAPIDE (dans un autre terminal)

### Health check
```bash
curl http://localhost:8000/health
```

### PrÃ©diction
```bash
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{"sepal_width": 3.5}'
```

RÃ©ponse attendue:
```json
{
  "sepal_width": 3.5,
  "sepal_length_pred": 5.9
}
```

### VÃ©rifier la DB
```bash
docker compose exec db psql -U iris -d irisdb -c "SELECT COUNT(*) FROM iris_clean;"
```

---

## ğŸ“Š FLUX DE DONNÃ‰ES

```
iris.csv (150 obs)
    â†“
[PREPROCESS] 
  - Normalise colonnes
  - Supprime NA/doublons
  - StandardScaler
    â†“
PostgreSQL (iris_clean table)
    â†“
[TRAIN]
  - RandomForestRegressor (n_estimators=200)
  - Target: sepal_length
  - Feature: sepal_width
  - Calcule MSE, RÂ²
    â†“
MLflow (tracking)
  - Log params + metrics
  - Enregistre modÃ¨le (Model Registry)
    â†“
[API FastAPI]
  - Charge modÃ¨le depuis MLflow
  - Route /predict
    â†“
Client (curl, Swagger, etc.)
```

---

## ğŸ“ SOUTENANCE - CHECKLIST

### Ã€ montrer
- [ ] `docker compose up --build` et attendre ~2-3 min
- [ ] **MLflow UI** : naviguer dans l'expÃ©rience `iris-sepal-regression`
  - Afficher les mÃ©triques (MSE â‰ˆ 0.12-0.15, RÂ² â‰ˆ 0.95-0.97)
  - Montrer le modÃ¨le enregistrÃ© (Model Registry)
- [ ] **Swagger API** (`http://localhost:8000/docs`)
  - Cliquer sur "Try it out" pour /predict
  - Envoyer une requÃªte avec `sepal_width: 3.5`
  - Afficher la rÃ©ponse
- [ ] **Base de donnÃ©es** (optionnel)
  - `docker compose exec db psql -U iris -d irisdb -c "SELECT * FROM iris_clean LIMIT 5;"`

### Ã€ expliquer
- [ ] Flux complet : CSV â†’ Preprocess â†’ DB â†’ Train â†’ MLflow â†’ API
- [ ] Isolation des services : chaque conteneur a un rÃ´le bien dÃ©fini
- [ ] DÃ©pendances respectÃ©es : `docker-compose.yml` contrÃ´le l'ordre
- [ ] Volumage persistant : `pgdata` (DB) et `mlruns` (artefacts)
- [ ] RÃ©seau Compose : services se parlent par nom (`db`, `mlflow`)
- [ ] Choix techniques :
  - **PostgreSQL** : SGBD robuste, backend officiel MLflow
  - **MLflow** : tracking + Model Registry + UI web
  - **FastAPI** : doc auto (Swagger), performance, typage Pydantic
  - **RandomForest** : baseline solide, pas d'hyperparamÃ¨tres critiques

### Diapos / Dossier technique
Ã€ rÃ©diger aprÃ¨s la dÃ©mo (structure dans README.md section "Dossier technique / slides")

---

## ğŸ” TROUBLESHOOTING RAPIDE

### "DB not reachable"
```bash
docker compose ps
# db doit Ãªtre en "healthy" status
```

### "No model versions found"
- VÃ©rifier que `train` s'est exÃ©cutÃ© : `docker compose logs train | tail -20`
- Attendre ~1-2 min aprÃ¨s "Preprocess done"

### "API rÃ©pond 500"
```bash
curl http://localhost:5000  # MLflow doit Ãªtre up
docker compose logs api     # Afficher les logs de l'API
```

### Relancer complÃ¨tement
```bash
docker compose down -v
docker compose up --build
```

---

## ğŸ“š STRUCTURE FINALE

```
data-pipeline4/
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ SYNTHÃˆSE.md
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ iris.csv
â”œâ”€â”€ launch.sh
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ .gitkeep
â”‚   â””â”€â”€ init.sql
â”œâ”€â”€ mlflow/
â”‚   â””â”€â”€ .gitkeep
â””â”€â”€ services/
    â”œâ”€â”€ preprocess/
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â”œâ”€â”€ preprocess.py
    â”‚   â””â”€â”€ requirements.txt
    â”œâ”€â”€ train/
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â”œâ”€â”€ train.py
    â”‚   â””â”€â”€ requirements.txt
    â””â”€â”€ api/
        â”œâ”€â”€ Dockerfile
        â”œâ”€â”€ main.py
        â””â”€â”€ requirements.txt
```

---

## ğŸ‰ NEXT STEPS

1. **Avant la soutenance** :
   - Test: `docker compose up --build` une fois complÃ¨tement
   - VÃ©rifier que MLflow UI et API rÃ©pondent
   - PrÃ©parer les diapos (justifications choix, perf, limites)

2. **Pistes d'amÃ©liorations** (bonus pour montrer de l'initiative) :
   - Cross-validation dans train.py
   - Hyperparameter tuning (GridSearch)
   - Versioning Production/Staging dans MLflow
   - UI bonus (page HTML avec formulaire)
   - Tests automatisÃ©s (pytest)

3. **Documentation supplÃ©mentaire** (si demandÃ©e) :
   - SchÃ©ma d'architecture (excalidraw, draw.io)
   - Logs d'une run complÃ¨te
   - Performances (temps d'exÃ©cution par service)

---

**Date**: 2026-02-02  
**Branch**: Amaury  
**Repo**: theotime2005/data-pipeline4

âœ¨ **Projet PRÃŠT pour prÃ©sentation!** ğŸ“
